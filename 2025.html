<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.3.0/font/bootstrap-icons.css">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">

    <style>
      html {
      scroll-behavior: smooth;
      padding-top: 4rem;
      }

      a {
      text-decoration: none;
      color: #3498db;
      }

      h3 {
      margin-top: 40px;
      }

      .h-title {
      font-variant: small-caps;
      margin-bottom: 0;
      }

      .h-line {
      visibility: hidden;
      }
      
      .h-line::after {
      visibility: visible;
      content: "";
      width: 120px;
      height: 1px;
      display: inline-block;
      background: #0d6efd;
      margin: 10px 10px;
      margin-top: 0px;
      }

      .talk-title {
      font-family: 'Roboto Slab', serif;
      }
    </style>

    <title>Workshop on Algorithms for Large Data (Online) 2025</title>
	
	<script language="JavaScript" type="text/JavaScript">
	function toggle(id) {
	if (document.getElementById){
		if (document.getElementById(id).style.display == "none"){
			document.getElementById(id).style.display = "flex";
			}
		else{
			document.getElementById(id).style.display = "none";
			}
		}
	}
	</script>
  </head>
  <body>
    
    <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">
      <div class="container">
        <a class="navbar-brand" href="#">WALDO 2025</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item"><a class="nav-link" href="#lk-info">Info</a></li>
            <li class="nav-item"><a class="nav-link" href="#lk-registration">Registration</a></li>
            <li class="nav-item"><a class="nav-link" href="#lk-speakers">Speakers</a></li>
            <li class="nav-item"><a class="nav-link" href="#lk-schedule">Schedule</a></li>
			<li class="nav-item"><a class="nav-link" href="#lk-posters">Posters</a></li>
			
          </ul>
        </div>
      </div>
    </nav>

    <h1 class="text-center py-4 text-light" style="background-color: #3CB371">Workshop on Algorithms for Large Data (Online) 2025</h1>
	
    <div class="container">
      <div class="row justify-content-md-center">
        <div class="col-9" id="main">
          <p id="lk-info">
            This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. 
			By bringing together experts from diverse fields, we seek to explore innovative approaches and shared challenges in this rapidly evolving area. 
			Due to a generous grant from <a href="https://www.sigact.org/">SIGACT</a>, the workshop is free to attend for all interested participants. 
          </p>

          <table class="table">
            <tbody>
              <tr>
                <th scope="row">What</th>
                <td>Workshop on Algorithms for Large Data (Online)</td>
              </tr>
              <tr>
                <th scope="row">When</th>
                <td>Monday, April 14 - Wednesday, April 16, 2025</td>
              </tr>
              <tr>
                <th scope="row">Where</th>
                <td>The workshop will be held virtually.</td>
              </tr>
            </tbody>
          </table>

          <!-- <p>INSERT LOGO HERE</p> -->

          <h3 class="h-title">Organizers</h3>
          <span class="h-line">Organizers</span>

          <ul class="list-group list-group-flush">
            <li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="https://web.stanford.edu/~gblanc/">Guy Blanc</a> (Stanford)</li>
            <li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="https://math.mit.edu/~shivamn/">Shivam Nadimpalli</a> (MIT)</li>
			<li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="https://quanquancliu.com/">Quanquan C. Liu</a> (Yale)</li>
            <li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="http://samsonzhou.github.io">Samson Zhou</a> (Texas A&#38;M University)</li>
          </ul>

          <h3 id="lk-registration" class="h-title">Registration</h3>
          <span class="h-line">Registration</span>

          <p>
		  <!--The workshop is now over. Thanks for your interest in attending WALDO25!-->
		  
		  <!--All interested participants are welcome to attend. There
            is no registration fee for the workshop, but registration is now closed and 
			the <a href="https://www.virtualchair.net/events/waldo-2021">landing page</a> is now available. 
			Please contact one of the organizers if you experience difficulties in accessing the platform. -->
		 
            All interested participants are welcome to attend. There is no registration fee for the workshop, but please
            register via <a href="https://docs.google.com/forms/d/1e69JrFREyO3Xi6DMIydv2soRgnDalua92VSfvcK3L3M/edit">this form</a>
            before <strong>April 7th, 2025</strong>, to receive the necessary access credentials.
        </p>

          <h3 id="lk-speakers" class="h-title">Speakers</h3>
          <span class="h-line">Speakers</span>

          <ul class="list-group list-group-flush">
		  
		    <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://maryamaliakbarpour.com/index.html">Maryam Aliakbarpour</a> (Rice) </li>
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sepehr.assadi.info/">Sepehr Assadi</a> (University of Waterloo) </li>
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="http://behnezhad.com/">Soheil Behnezhad</a> (Northeastern) </li>			
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://jbrakensiek.github.io/">Josh Brakensiek</a> (UC Berkeley) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://mbraverm.princeton.edu/">Mark Braverman</a> (Princeton) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://cskarthikcs.github.io/index.html">Karthik C.S.</a> (Rutgers) </li>
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://ccanonne.github.io/">Cl&eacute;ment Canonne</a> (University of Sydney) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.di.ens.fr/~vcohen/">Vincent Cohen-Addad</a> (Google Research NYC) </li>
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/sumegha-garg/home">Sumegha Garg</a> (Rutgers) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://cs.uwaterloo.ca/~elena-g/">Elena Grigorescu</a> (University of Waterloo) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://cs.nyu.edu/~anupamg/">Anupam Gupta</a> (NYU) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.meghalgupta.com/">Meghal Gupta</a> (UC Berkeley) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a> (MIT) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a> (EPFL) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.cis.upenn.edu/~sanjeev/">Sanjeev Khanna</a> (UPenn) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.cs.princeton.edu/~kothari/">Pravesh Kothari</a> (Princeton) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://people.csail.mit.edu/jlange/">Jane Lange</a> (MIT) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/slobodan-mitrovic/home">Slobodan Mitrovi&cacute;</a> (UC Davis) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://people.csail.mit.edu/ronitt/">Ronitt Rubinfeld</a> (MIT) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.cs.columbia.edu/~rocco/">Rocco Servedio</a> (Columbia) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://home.ttic.edu/~madhurt/">Madhur Tulsiani</a> (TTIC) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://web.eecs.umich.edu/~nswein/">Nicole Wein</a> (Michigan) </li>
            <!--<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="http://www.mit.edu/~mahabadi/">Sepideh Mahabadi</a> (TTIC) </li>-->
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://cs-people.bu.edu/sofya/">Sofya Raskhodnikova</a> (Boston University) </li>
            <li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a> (Carnegie Mellon) </li>
			<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.cs.princeton.edu/~hy2/">Huacheng Yu</a> (Princeton) </li>
			
          </ul>        
		  
		  
		   <h3 id="lk-schedule" class="h-title">Schedule</h3>
          <span class="h-line">Schedule (Tentative)</span>

          <ul class="nav nav-tabs" id="myTab" role="tablist">
<li class="nav-item" role="presentation">
<button class="nav-link active" id="day1-tab" data-bs-toggle="tab" data-bs-target="#day1" type="button" role="tab" aria-controls="day1" aria-selected="true">Day 1 (April 14)</button>
</li>
<li class="nav-item" role="presentation">
<button class="nav-link" id="day2-tab" data-bs-toggle="tab" data-bs-target="#day2" type="button" role="tab" aria-controls="day2" aria-selected="false">Day 2 (April 15)</button>
<li class="nav-item" role="presentation">
<button class="nav-link" id="day3-tab" data-bs-toggle="tab" data-bs-target="#day3" type="button" role="tab" aria-controls="day3" aria-selected="false">Day 3 (April 16)</button>
</li>
</ul>

<div class="tab-content" id="myTabContent">
<!-- Day 1 -->
<div class="tab-pane fade show active" id="day1" role="tabpanel" aria-labelledby="day1-tab">
<div class="container-fluid">
<div class="row my-3"><div class="col-2">12:00 pm ET</div>
<div class="col"><span class="talk-title">Opening Remarks by SIGACT</span> </span>
</div></div><hr />

<!-- Talk 2 -->
<div class="row my-3">
<div class="col-2">12:05pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://cs.nyu.edu/~anupamg/">Anupam Gupta</a>: The Price of Explainability for Clustering</span>
[<a href="javascript:toggle('AG')">Abstract</a>]
<div id="AG" style="display: none">
<p>
A recent line of work asks: given a clustering optimization problem (like k-means or k-medians), how does the cost of the best explainable clustering compare to that of the best clustering? 
We consider the model where a clustering is called explainable if it can be defined by a tree of axis-parallel cuts. Building on a recent sequence of works, we pin down the optimal price of explainability 
for k-median clustering, and also improve on existing results for k-means clustering. In this talk, I will discuss some of these results (both ours and prior results), the techniques underlying them, 
and point out some of the open questions in the area. 
</br></br>
This talk is based on this paper (<a href="https://arxiv.org/abs/2304.09743">https://arxiv.org/abs/2304.09743</a>) with Madhusudhan Pittu (CMU), Ola Svensson (EPFL), and Rachel Yuan (CMU).
</p>
</div>
</div>
</div>
<hr />

<!-- Talk 3 -->
<div class="row my-3">
<div class="col-2">12:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://mbraverm.princeton.edu/">Mark Braverman</a>: Optimality of Frequency Moment Estimation</span> 
[<a href="javascript:toggle('DW')">Abstract</a>]
<div id="DW" style="display: none">
<p>
Estimating the second frequency moment of a stream up to (1
± ε) multiplicative error requires at most O(log n / ε²) bits of
space, due to a seminal result of Alon, Matias, and Szegedy. It is
also known that at least Ω(log n + 1/ε²) space is needed.
We prove a tight lower bound of Ω(log(nε²) / ε²) for all ε = Ω(1/√n).
When ε > n^(-1/2 + c), where c > 0, our lower bound matches the
classic upper bound of AMS. For smaller values of ε, we also introduce
a revised algorithm that improves the classic AMS bound and matches
our lower bound.
</br></br>
Our lower bound also applies to the more general problem of p-th
frequency moment estimation for the range of p in (1, 2], providing a
tight bound in the only remaining range to settle the optimal space
complexity of estimating frequency moments.
</br></br>
Based on a joint work with Or Zamir
</p>
</div>
</div>
</div>
<hr />

<!-- Break -->
<div class="row my-3">
<div class="col-2">12:55 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />

<!-- Talk 4 -->
<div class="row my-3">
<div class="col-2">13:10 pm ET
</div>
<div class="col"><span class="talk-title"><a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a>: TBD</span>  
[<a href="javascript:toggle('SR')">Abstract</a>]
<div id="SR" style="display: none">
<p>

</p>
</div>
</div>
</div>
<hr />

<!-- Talk 5 -->
<div class="row my-3">
<div class="col-2">13:35 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://sites.google.com/view/slobodan-mitrovic/home">Slobodan Mitrovi&cacute;</a>: Computing Graph Cuts Privately</span>
[<a href="javascript:toggle('SM')">Abstract</a>]
<div id="SM" style="display: none">
<p>
With the increasing availability of publicly shared statistics derived from private datasets, safeguarding users' personal information has become crucial. Differential privacy (DP) 
has emerged as a widely adopted framework for quantifying the level of individual privacy an algorithm preserves.
</br></br>
Over the past decade, numerous fundamental algorithms have been studied within the context of DP. This presentation will focus on 
recent advances in the differentially private computation of graph cuts. We will also touch on multiple interesting open problems in this area of research.
</p>
</div>
</div>
</div>
<hr />


<!-- Break -->
<div class="row my-3">
<div class="col-2">14:00 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Monday Poster Session</span> <small class="text-muted"></small></span>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">15:00 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://home.ttic.edu/~madhurt/">Madhur Tulsiani</a>: TBD</span> 
[<a href="javascript:toggle('MT')">Abstract</a>]
<div id="MT" style="display: none"> 
<p>

</p>
</div>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">15:25 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.meghalgupta.com/">Meghal Gupta</a>: Stream-Decodable Error-Correcting Codes</span>
[<a href="javascript:toggle('MG')">Abstract</a>]
<div id="MG" style="display: none">
<p>
In the standard noisy communication model, Alice encodes a message using an error-correcting code and sends it to Bob, 
who decodes it after receiving the entire message and storing it in memory. In this talk, we'll explore what happens when Bob 
doesn't have enough memory to store the whole message and must instead decode it bit by bit as it arrives. We'll define what it 
means for a code to be stream-decodable and present nearly matching upper and lower bounds on the code length required in this setting.
</br></br>
This is based on joint works with Venkat Guruswami, Mihir Singhal, and Rachel Zhang.
</p>
</div>
</div>
</div>
<hr />


<!-- Break -->
<div class="row my-3">
<div class="col-2">15:50 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">16:05 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.cs.columbia.edu/~rocco/">Rocco Servedio</a>: Is Nasty Noise Actually Harder than Malicious Noise?</span>
[<a href="javascript:toggle('RS')">Abstract</a>]
<div id="RS" style="display: none">  
<p>
We consider the relative abilities and limitations of computationally efficient algorithms for learning in the presence of noise, under two well-studied and challenging adversarial noise models for learning Boolean functions:
</br>
</br>
*malicious noise, in which an adversary can arbitrarily corrupt a random subset of examples given to the learner; and
</br>
*nasty noise, in which an adversary can arbitrarily corrupt an adversarially chosen subset of examples given to the learner.
</br>
</br>
We consider both the distribution-independent and fixed-distribution settings. Our main results highlight a dramatic difference between these two settings:
</br>
1. For distribution-independent learning, we prove a strong equivalence between the two noise models: If a class C of functions is efficiently learnable in the presence of η-rate malicious noise, then it is also efficiently learnable in the presence of η-rate nasty noise.
</br>
2. In sharp contrast, for the fixed-distribution setting we show an arbitrarily large separation: Under a standard cryptographic assumption, for any arbitrarily large value r there exists a concept class for which there is a ratio of r between the rate η_malicious of 
malicious noise that polynomial-time learning algorithms can tolerate, versus the rate η_nasty of nasty noise that such learning algorithms can tolerate.
</br>
</br>
Joint work with Guy Blanc, Yizhi Huang, and Tal Malkin.
</p> 
</div>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">16:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.cs.princeton.edu/~kothari/">Pravesh Kothari</a>: The Quasi-polynomial Low-Degree Conjecture is False</span>
[<a href="javascript:toggle('PK')">Abstract</a>]
<div id="PK" style="display: none">
<p>
There is a growing body of work on proving hardness results for average-case optimization problems by bounding 
the <i>low-degree likelihood ratio</i> (LDLR) --- a quantitative estimate of the closeness of low-degree moments --- between a null distribution 
and a related planted distribution. Such hardness results are now ubiquitous for foundational problems in algorithms, statistics, and cryptography.
</br></br>
This line of work is supported by the low-degree conjecture of Hopkins, which postulates that a vanishing degree-D LDLR implies 
the absence of any noise-tolerant distinguishing algorithm with runtime n^{\widetilde{O}(D)} whenever 1) the null distribution is 
product on $\{0,1\}^{\binom{n}{k}}$, and 2) the planted distribution is permutation invariant, that is, invariant under any relabeling $[n] \rightarrow [n]$.
</br></br>
In this talk, I'll present our refutation of this conjecture. Specifically, we show that for all sufficiently small $\epsilon>0$ and 
any $k\geq 2$, there is a permutation-invariant planted distribution on $\{0,1\}^{\binom{n}{k}}$ that has a vanishing degree-$n^{1-O(\epsilon)}$ 
LDLR with respect to the uniform distribution on $\{0,1\}^{\binom{n}{k}}$ even as a $n^{O(\log^{1/(k-1)}(n))}$-time algorithm solves 
the corresponding $\epsilon$-noisy distinguishing problem. Our construction relies on algorithms for list decoding for noisy polynomial interpolation in the high-error regime.
</br></br>
We also give another construction of a pair of planted and (a non-product) null distributions on $\R^{n \times n}$ with a 
vanishing $n^{\Omega(1)}$-degree LDLR even as simply the largest eigenvalue serves as an efficient noise-tolerant distinguisher. 
Our results suggest that while a vanishing LDLR bound may still be interpreted as evidence of hardness, developing a theory of average-case 
complexity based on such heuristics requires a more careful approach. 
</p>
</div>
</div>
</div>
<hr />
</div>
</div> <!-- Day -->

<!-- Day 2 -->
<div class="tab-pane fade" id="day2" role="tabpanel" aria-labelledby="day2-tab">
<div class="container-fluid">
<!-- Talk 1 -->
<div class="row my-3">
<div class="col-2">12:00 pm ET
</div>
<div class="col"><span class="talk-title">Opening Remarks by Steering Committee</span> </span>
</div>
</div>
<hr />

<!-- Talk 2 -->
<div class="row my-3">
<div class="col-2">12:05 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a>: TBD</span> 
[<a href="javascript:toggle('MK')">Abstract</a>]
<div id="MK" style="display: none">
<p>

</p>
</div>
</div>
</div>
<hr />

<!-- Talk 3 -->
<div class="row my-3">
<div class="col-2">12:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.cis.upenn.edu/~sanjeev/">Sanjeev Khanna</a>: Correlation Clustering and (De)Sparsification: Graph Sketches Can Match Classical Algorithms</span> 
[<a href="javascript:toggle('SK')">Abstract</a>]
<div id="SK" style="display: none">
<p>
Correlation clustering is a widely-used approach for clustering large data sets based only on pairwise similarity information. 
In recent years, there has been a steady stream of better and better classical algorithms for approximating this problem. 
Meanwhile, another line of research has focused on porting the classical advances to various sublinear algorithm models, including semi-streaming, 
Massively Parallel Computation (MPC), and distributed computing. Yet, these latter works typically rely on 
ad-hoc approaches that may not always keep up with advances in improved approximation ratios achieved by classical algorithms. 
This raises the following natural question: can the gains made by classical algorithms for correlation clustering be ported over to 
sublinear algorithms in a black-box manner? We answer this question in the affirmative via the paradigm of graph de-sparsification that may be of independent interest.
</br></br>
This is joint work with Sepehr Assadi and Aaron (Louie) Putterman.
</p>
</div>
</div>
</div>
<hr />

<!-- Break -->
<div class="row my-3">
<div class="col-2">12:55 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />

<!-- Talk 4 -->
<div class="row my-3">
<div class="col-2">13:10 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://web.eecs.umich.edu/~nswein/">Nicole Wein</a>: Covering Approximate Shortest Paths with DAGs</span>
[<a href="javascript:toggle('NW')">Abstract</a>]
<div id="NW" style="display: none">
<p>
I will talk about the new notion of a "DAG cover" that we define. It is a directed analog of a tree cover, 
which is closely related to a probabilistic tree embedding. A DAG cover of a general directed graph G is a 
small collection of DAGs so that for all pairs of vertices s,t, some DAG in the collection provides low distortion for dist(s,t). 
I will discuss upper and lower bounds for DAG covers in various parameter regimes, and pose some open problems.
</br></br>
Joint work with Sepehr Assadi and Gary Hoppenworth
</p>
</div>
</div>
</div>
<hr />

<!-- Talk 5 -->
<div class="row my-3">
<div class="col-2">13:35 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://cskarthikcs.github.io/index.html">Karthik C.S.</a>: Near-Optimal Lowe Bound for Parameterized Euclidean k-means</span>
[<a href="javascript:toggle('KC')">Abstract</a>]
<div id="KC" style="display: none">
<p>
The Euclidean k-means problem is a fundamental problem in computational geometry, where given a set of points in high-dimensional space, 
the goal is to find k representative points so as to minimize the sum of the squared distances from each point to its closest representative. 
In seminal works, de la Vega, Karpinski, Kenyon, and Rabani [STOC'03] and Kumar, Sabharwal, and Sen [JACM'10] showed how to obtain a (1+eps)-approximation for 
high-dimensional Euclidean k-means in time poly(nd) 2^(k/eps)^{O(1)}. 
</br></br>
In this talk, we introduce a new fine-grained hypothesis called Exponential Time for Expanders Hypothesis (XXH), which roughly asserts that there are no 
non-trivial exponential-time approximation algorithms for the vertex cover problem on near-perfect vertex expanders. Assuming XXH, 
we close the above long line of work on approximating Euclidean k-means by showing that there is no poly(nd) 2^(o(k/eps)) time algorithm 
achieving a (1+eps)-approximation for k-means in Euclidean space. 
This lower bound is tight as it matches the algorithm given by Feldman, Monemizadeh, and Sohler [SoCG'07] up to log factors in the exponent.
</p>
</div>
</div>
</div>
<hr />

<!-- Extra Talk -->
<div class="row my-3">
<div class="col-2">14:00 pm ET
</div>
<div class="col"><span class="talk-title"><a href="http://behnezhad.com/">Soheil Behnezhad</a>: TBD</span>
[<a href="javascript:toggle('SB')">Abstract</a>]
<div id="SB" style="display: none">
<p>

</p>
</div>
</div>
</div>
<hr />

<!-- Break -->
<div class="row my-3">
<div class="col-2">14:25 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Break</span> <small class="text-muted">(35 mins)</small></span>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">15:00 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.di.ens.fr/~vcohen/">Vincent Cohen-Addad</a>: TBD</span>
[<a href="javascript:toggle('VC')">Abstract</a>]
<div id="VC" style="display: none">
<p>

</p>
</div>
</div>
</div>
<hr />

<div class="row my-3">
<div class="col-2">15:25 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://cs.uwaterloo.ca/~elena-g/">Elena Grigorescu</a>: Differential Privacy and Sublinear-Time are Incompatible Sometimes</span>
[<a href="javascript:toggle('EG')">Abstract</a>]
<div id="EG" style="display: none"> 
<p>
Differential privacy and sublinear algorithms have both been well-studied paradigms in big data analysis. Although recent works have shown the existence of differentially private 
sublinear algorithms for many problems, little is known regarding hardness results on these algorithms. In this paper, we initiate the study of lower bounds for problems that 
aim for both differentially-private and sublinear-time algorithms. Our main result is the incompatibility of both the desiderata in the general case. 
In particular, we prove that a simple problem based on one-way marginals yields both a differentially-private algorithm, as well as a sublinear-time algorithm, 
but does not admit a ``strictly'' sublinear-time algorithm that is also differentially private. 
</br></br>
Joint work with <a href="https://www.cs.purdue.edu/homes/jblocki/">Jeremiah Blocki</a>, <a href="https://hendrik-fichtenberger.de/">Hendrik Fichtenberger</a>, 
<a href="https://tamalikamukherjee.com/">Tamalika Mukherjee</a>.
</p>
</div>
</div>
</div>
<hr />
<!-- Break -->
<div class="row my-3">
<div class="col-2">15:50 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">16:05 pm ET
</div>
<div class="col"><span class="talk-title"> <a href="https://people.csail.mit.edu/jlange/">Jane Lange</a>: TBD</span> 
[<a href="javascript:toggle('JL')">Abstract</a>]
<div id="JL" style="display: none"> 
<p>
</p>
</div>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">16:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://jbrakensiek.github.io/">Josh Brakensiek</a>: Redundancy Is All You Need (for Sparsification)</span>
[<a href="javascript:toggle('JB')">Abstract</a>]
<div id="JB" style="display: none">
<p>
The seminal work of Benczúr and Karger demonstrated cut sparsifiers of near-linear size, with several applications throughout theoretical computer science. Subsequent extensions have yielded sparsifiers for hypergraph cuts and more recently linear codes over Abelian groups. 
A decade ago, Kogan and Krauthgamer asked about the sparsifiability of arbitrary constraint satisfaction problems (CSPs). For this question, a trivial lower bound is the size of a non-redundant CSP instance, which admits, for each constraint, an assignment satisfying only 
that constraint (so that no constraint can be dropped by the sparsifier). For graph cuts, spanning trees are non-redundant instances.
</br></br>
Our main result is that redundant clauses are sufficient for sparsification: for any CSP predicate R, every unweighted instance of CSP(R) has a sparsifier of size at most its non-redundancy (up to polylog factors). For weighted instances, we similarly pin down the sparsifiability 
to the so-called chain length of the predicate. These results precisely determine the extent to which any CSP can be sparsified. A key technical ingredient in our work is a novel application of the entropy method from Gilmer's recent breakthrough on the union-closed sets conjecture.
</br></br>
As an immediate consequence of our main theorem, a number of results in the non-redundancy literature immediately extend to CSP sparsification. We also contribute new techniques for understanding the non-redundancy of CSP predicates. In particular, we give an explicit family of 
predicates whose non-redundancy roughly corresponds to the structure of matching vector families in coding theory. By adapting methods from the matching vector codes literature, we are able to construct an explicit predicate whose non-redundancy has a non-integral exponent. 
</br></br>
Joint work with Venkatesan Guruswami.
</p>
</div>
</div>
</div>
<hr />
</div>
</div> <!-- Day -->
<!-- Day 3 -->
<div class="tab-pane fade" id="day3" role="tabpanel" aria-labelledby="day3-tab">
<div class="container-fluid">
<!-- Talk 1 -->
<div class="row my-3">
<div class="col-2">12:00 pm ET
</div>
<div class="col"><span class="talk-title">Opening Remarks by Organizers</span> </span>
</div>
</div>
<hr />
<!-- Talk 2 -->
<div class="row my-3">
<div class="col-2">12:05 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://people.csail.mit.edu/ronitt/">Ronitt Rubinfeld</a>: TBD</span>
[<a href="javascript:toggle('RR')">Abstract</a>]
<div id="RR" style="display: none">
<p>
</p>
</div>
</div>
</div>
<hr />
<!-- Talk 3 -->
<div class="row my-3">
<div class="col-2">12:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://cs-people.bu.edu/sofya/">Sofya Raskhodnikova</a>: Fully Dynamic Algorithms for Graphs with Edge Differential Privacy</span>
[<a href="javascript:toggle('MB')">Abstract</a>]
<div id="MB" style="display: none">
<p>
We study differentially private algorithms for analyzing graphs in the challenging setting of continual release with fully dynamic updates, where edges are inserted and deleted over time, 
and the algorithm is required to update the solution at every time step. Previous work has presented differentially private algorithms for many graph problems that can handle insertions only or deletions only 
(called partially dynamic algorithms) and obtained some hardness results for the fully dynamic setting. The only algorithms in the latter setting were for the edge count, given by Fichtenberger, Henzinger, and Ost (ESA '21), 
and for releasing the values of all graph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML '23). We provide the first differentially private and fully dynamic graph algorithms for several 
other fundamental graph statistics (including the triangle count, the number of connected components, the size of the maximum matching, and the degree histogram), analyze their error, and show strong 
lower bounds on the error for all algorithms in this setting. 
</br></br>
In the talk, we will discuss two variants of edge differential privacy for fully dynamic graph algorithms and our current understanding of the error achievable under both variants: event-level and item-level. 
Under the former notion, two graph update sequences are considered neighboring  if, roughly speaking, they differ in at most one update; under the latter notion, they can differ only in updates pertaining to one edge. 
Differential privacy requires that for any two neighboring inputs, the output distributions of the algorithm are close. We give upper and lower bounds on the error of both---event-level and item-level---fully dynamic algorithms 
for several fundamental graph problems. No fully dynamic algorithms that are private at the item-level (the more stringent of the two notions) were known before. In the case of item-level privacy, for several problems, 
our algorithms match our lower bounds.
</br></br>
Joint work with Teresa Anna Steiner
</p>
</div>
</div>
</div>
<hr />
<!-- Break -->
<div class="row my-3">
<div class="col-2">12:55 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />
<!-- Talk 4 -->
<div class="row my-3">
<div class="col-2">13:10 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a>: TBD</span>
[<a href="javascript:toggle('PI')">Abstract</a>]
<div id="PI" style="display: none">
<p>
</p>
</div>
</div>
</div>
<hr />
<!-- Talk 5 -->
<div class="row my-3">
<div class="col-2">13:35 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://sites.google.com/view/sumegha-garg/home">Sumegha Garg</a>: TBD</span>
[<a href="javascript:toggle('SG')">Abstract</a>]
<div id="SG" style="display: none"> 
<p>
</p>
</div>
</div>
</div>
<hr />
<!-- Break -->
<div class="row my-3">
<div class="col-2">14:00 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Wednesday Poster Session</span> <small class="text-muted"></small></span>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">15:00 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://sepehr.assadi.info/">Sepehr Assadi</a>: TBD</span> 
[<a href="javascript:toggle('SA')">Abstract</a>]
<div id="SA" style="display: none"> 
<p>
</p>
</div>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">15:25 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://www.cs.princeton.edu/~hy2/">Huacheng Yu</a>: TBD</span>
[<a href="javascript:toggle('HY')">Abstract</a>]
<div id="HY" style="display: none"> 
<p>
</p>
</div>
</div>
</div>
<hr />
<!-- Break -->
<div class="row my-3">
<div class="col-2">15:50 pm ET
</div>
<div class="col text-center">
<span class="fs-3"><span class="talk-title">Coffee Break</span> <small class="text-muted">(15 mins)</small></span>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">16:05 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://maryamaliakbarpour.com/index.html">Maryam Aliakbarpour</a>: Leveraging Predictions for Efficient Hypothesis Testing in Discrete Distributions</span> 
[<a href="javascript:toggle('MA')">Abstract</a>]
<div id="MA" style="display: none"> 
<p>
Prior knowledge—whether from historical data, domain expertise, or predictive models—can enhance statistical inference by reducing sample complexity. 
We introduce a general framework for leveraging such predictions and apply it to hypothesis testing for discrete distributions.  In the standard setting, 
optimal sample complexity bounds are known for identity, closeness, and uniformity testing. We show that access to a predicted distribution can reduce the required samples, 
with gains determined by its total variation distance from the true distribution. Our algorithms are adaptive, adjusting to prediction accuracy without prior calibration, 
and robust, never exceeding the standard sample complexity when predictions are uninformative. We establish information-theoretic lower bounds confirming optimality and 
present experimental results demonstrating significant practical improvements.
</p>
</div>
</div>
</div>
<hr />
<div class="row my-3">
<div class="col-2">16:30 pm ET
</div>
<div class="col"><span class="talk-title"><a href="https://ccanonne.github.io/">Cl&eacute;ment Canonne</a>: Better Private Distribution Testing by Leveraging Unverified Auxiliary Data </span>
[<a href="javascript:toggle('CC')">Abstract</a>]
<div id="CC" style="display: none">
<p>
We extend the framework of augmented distribution testing (Aliakbarpour, Indyk, Rubinfeld, and Silwal, NeurIPS 2024) to the differentially private setting. 
This captures scenarios where a data analyst must perform hypothesis testing tasks on sensitive data, but is able to leverage prior knowledge 
(public, but possibly erroneous or untrusted) about the data distribution.
</br></br>
We design private algorithms in this augmented setting for three flagship distribution testing tasks, uniformity, identity, and closeness testing, 
whose sample complexity smoothly scales with the claimed quality of the auxiliary information. We complement our algorithms with 
information-theoretic lower bounds, showing that their sample complexity is optimal (up to logarithmic factors).
</br></br>
Joint work with Maryam Aliakbarpour, Arnav Burudgunte, and Ronitt Rubinfeld.
</p>
</div>
</div>
</div>
<hr />
</div>
</div> <!-- Day -->

<h3 id="lk-posters" class="h-title">Posters</h3>
<span class="h-line">Posters</span></br>
Monday Poster Session:
<ul class="list-group list-group-flush">
		  
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://anaymehrotra.com/">Anay Mehrotra</a> (Yale): Can SGD Select Good Fishermen? Local Convergence Under Self-Selection Biases and Beyond </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://eg314159.github.io/">Elena Gribelyuk</a> (Princeton): <a href="https://waldo-workshop.github.io/2025/P02--poster.pdf">Lifting Linear Sketches: Optimal Bounds and Adversarial Robustness</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://emiletimothy.net/">Emile Anand</a> (Georgia Tech): The Structural Complexity of Matrix-Vector Multiplication </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://honghlin.github.io/">Honghao Lin</a> (Carnegie Mellon) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://student.cs.uwaterloo.ca/~jsundare/">Janani Sundaresan</a> (University of Waterloo) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://jusyc.github.io/">Justin Y. Chen</a> (MIT): <a href="https://waldo-workshop.github.io/2025/P06--poster.pdf">Statistical-Computational Trade-offs for Hypothesis Selection</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.kstavrop.com/">Konstantinos Stavropoulos</a> (UT Austin): <a href="https://waldo-workshop.github.io/2025/P07--poster.pdf">Efficiently Certifiable Guarantees for Learning with Distribution Shift</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://web.eecs.umich.edu/~lilyxy/">Lily Wang</a> (Michigan) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://lydiazakynthinou.com/">Lydia Zakynthinou</a> (UC Berkeley): <a href="https://waldo-workshop.github.io/2025/P09--poster.pdf">Dimension-free Private Mean Estimation</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://nmenand.github.io/">Nicolas Menand</a> (UPenn): Streaming and Massively Parallel Algorithms for Euclidean Max-Cut </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://prashantianderson.github.io/">Prashanti Anderson</a> (MIT): Sample-Optimal Private Regression in Polynomial Time </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://rheaj99.github.io/">Rhea Jain</a> (UIUC) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://scholar.google.com/citations?user=19evEwcAAAAJ&hl=en">Shourya Pandey</a> (UT Austin) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://vaidehi8913.github.io/">Vaidehi Srinivas</a> (Northwestern) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://willdguo.github.io/">William Guo</a> (UPenn): <a href="https://waldo-workshop.github.io/2025/P15--poster.pdf">Oja’s Algorithm for Streaming PCA: Spectral Guarantees for Sparse Matrices</a> </li>
</ul>
Wednesday Poster Session:
<ul class="list-group list-group-flush">
		  
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://almaghafari.github.io/">Alma Ghafari</a> (Northeastern) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://abrandenberger.github.io/">Anna Brandenberger</a> (MIT) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/chen-wang/home">Chen Wang</a> (Rice and Texas A&#38;M University) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://debanujnayak.github.io/">Debanuj Nayak</a> (Boston University): Differentially Private Multi-Sampling from Distributions </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://dyukha.github.io/">Dmitrii Avdiukhin</a> (Northwestern): <a href="https://waldo-workshop.github.io/2025/P20--poster.pdf">Embedding Dimension of Contrastive Learning and <i>k</i>-Nearest Neighbors</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://felix-zhou.com/">Felix Zhou</a> (Yale): <a href="https://waldo-workshop.github.io/2025/P21--poster.pdf">Private Training & Synthetic Data via DP Clustering</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://jimz7.github.io/">Jinze Zhao</a> (UT Austin) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://yinj66.github.io/">Junze Yin</a> (Rice) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://maoyuans.github.io/">Maoyuan "Raymond" Song</a> (Purdue): <a href="https://waldo-workshop.github.io/2025/P24--poster.pdf">Learning-Augmentation for Online Convex Covering and Concave Packing</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/nithishkumarkumar">Nithish Kumar Kumar</a> (Purdue) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/psawettamalya">Pachara Sawettamalya</a> (Princeton): <a href="https://waldo-workshop.github.io/2025/P26--poster.pdf">Strong XOR Lemma for Information Complexity</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/pooja-kulkarni/home">Pooja Kulkarni</a> (UIUC) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://sites.google.com/view/shenghaoxie/">Shenghao Xie</a> (Texas A&#38;M University): <a href="https://waldo-workshop.github.io/2025/P28--poster.pdf">Perfect Sampling in Turnstile Streams Beyond Small Moments</a> </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://www.themisharis.com/">Themistoklis Haris</a> (Boston University) </li>
<li class="list-group-item"><i class="bi bi-caret-right"></i> <a href="https://scholar.google.com/citations?user=DJ6cfPgAAAAJ&hl=en">Wei Zhang</a> (MIT): <a href="https://waldo-workshop.github.io/2025/P30--poster.pdf">Optimal k-secretary with Logarithmic Memory</a> </li>
</ul>
		  
		  <h3 id="lk-support" class="h-title">Support</h3>
          <span class="h-line">Support</span>
		  
		  <p>
		  WALDO 2025 is generously supported by a community grant from <a href="https://www.sigact.org/">SIGACT</a>. 
		  Web design by <a href="https://www.cs.cmu.edu/~preisben/">Pedro Paredes</a>. 
		  </p>
		  
		  <h3 class="h-title">Steering Committee</h3>
          <span class="h-line">Steering Committee</span>

          <ul class="list-group list-group-flush">
            <li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="http://aineshbakshi.com">Ainesh Bakshi</a> (MIT)</li>
			<li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="https://www.cs.jhu.edu/~vova/">Vladimir Braverman</a> (Johns Hopkins)</li>
            <li class="list-group-item"><i class="bi bi-person-fill"></i> <a href="http://rajeshjayaram.com">Rajesh Jayaram</a> (Google Research NYC)</li>
          </ul>
		  
          </div>
		  
		  
		  
		  
        </div>		
      </div>	  
    </div>

          

    <!-- Copyright -->
  <div class="text-center p-5" style="background-color: rgba(0, 0, 0, 0.05);">
  </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>
  </body>
</html>